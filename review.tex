\newpage
\section{Обзор методов фильтрации спама}
\label{review}
%Сюда я скопирую раздел из курсовой работы 4го курса
\section{Обзор методов машинного обучения}
\subsection{Обучение по прецедентам}
Пусть есть неизвестная функция $F: X -> Y$, переводящая объекты
множества $X$ в объекты множества $Y$, причем для некторых $x_1, x_2, ... x_n$ известны соответсвующие им значения $y_1 = F(x_1), y_2 = F(x_2), ... y_n = F(x_n)$.
Необходимо построить функцию $F^*(X)$, наилучшим образом приближающую $F(X)$.

Под фразой \textbf{наилучшим образом приближает} подразумевается, что для некоторого функционала качества $\mu(y, y')$, матожидание
\begin{equation}
\label{}
E\mu(F(x), F^*(x)), x \in X
\end{equation}
будет минимальным.

В качестве $\mu(y, y')$ можно брать например модуль разности, квадрат разности и т. п.

\textbf{Методом обучения} называется проецесс построения $F^*(x)$ по известным парам $(x_1, y_1), (x_2, y_2), ... (x_n, y_n)$. Множество таких пар называется \textbf{обучающей разности}

Построенную функцию $F^*(x)$ часто также называют \textbf{алгоритмом}, подразумевая что она должна быть эффективно вычислима на компьютере.

Задача построения такого алгоритма не может быть решена точно, т. к. неизвестна природа исходной функции $F(x)$. Кроме того минимизация $\mu$ на обучающей выборке не обязательно приводит к тому, что и на новых обхектах из $X$ значение $\mu$ также будет мало.
%TODO смотри пример(рисунок, переобучение)

Процесс построения модели можно также рассматривать как выбор конкретного значения функции $F^*(x)$ из семейства $F^*(x, \pi)$. Значение выбранного параметра $\pi$ в таком случае называется \textbf{моделью} для алгоритма
$F^*(x)$

В целом процесс решения задачи машинного обучения называют \textbf{машинным обучением} или \textbf{обучением по прецедентам}

В зависимости от вида множества $Y$, задача может являться задачей классификации(множество конечно) или регрессии(множество бесконечно).
Задача фильтрации спама по своей сути является задачей классификации с двумя классами.

Объекты из множества $X$ обычно рассматривают как вектор в некотором $N$-мерном пространстве. Если изначально объекты имеют более сложную структуру(например текст, аудиозапись), то ее некоторым способом представляют в виде вектора. Элементы таких векторов называются \textbf{признаками}.

\subsection{Скользящий контроль}
Так как вид функции $F(X)$  неизвестен, то прямо посчитать значение матожидания \ref{matozh} не возможным. Для того чтобы хоть как-то оценить качество построенного алгоритма обычно пользуются методом \textbf{скользащего контроля}. Метод заключается в следующем: первоначальная обучающая выборка делится на несколько частей. Обучение проводится по очереди на каждой из частей, а значение $\mu$ оценивается по оставшимся частям.

Итоговую оценку $\mu$ считают как
\begin{equation}
\mu' = 1/n\sum_1^n{\mu_i}, i \in 1..n
\end{equation}

\subsection{Байесовские методы}
\subsubsection{Наивный байесовский классификатор}
\subsection{Нейронные сети}
\subsection{Решающие деревья}
%TODO обязательно рисунок!
Решающие деревья представляют собой идейно достаточно простой метод обучения: строится дерево, в узлах которого ставятся некоторые предикаты(простые пороговые решающие правила). В листах этого дерева записываются значения $F^*(x)$, соответсвующие значеиниям предикатов.

Обычно такие деревья склонны к переобучению, однако существуют закрытые реализации алгоритмов классификации, в которых деревья используются в качестве базовых алгоритмов для более сложных алгоримов.
\ref{treenet} \ref{treenet}

\subsection{Метод опорных векторов}

\section{Обзор существующих открытых систем фильтрации спама}
\subsection{spamassassin}
\subsection{spambrobe}
\subsection{dspam}

